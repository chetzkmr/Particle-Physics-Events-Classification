# -*- coding: utf-8 -*-
"""Physics particle experiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-e57XGhOM9lqp6B-qV-NGMdsTZJfMbnE

# Prediction of physics particle event experiment
"""

# Commented out IPython magic to ensure Python compatibility.
#importing libraries
import numpy as np
import pandas as pd
import seaborn as sns
sns.set()
import matplotlib.pyplot as plt
# %matplotlib inline

#importing the data
df=pd.read_csv('/content/drive/MyDrive/Placement project mar/Copy of Data.csv')
df.head()

df.shape

df.isnull().sum()

"""No null values avialable"""

df.describe()

df.describe(include=['object'])

df.nunique()

#Checking for duplicate values
df.duplicated().sum()

df.dtypes

"""All the data types are numerical"""

df.columns

# Pie chart to observe the distribution of label column
labels = df['Label'].value_counts().index
sizes = df['Label'].value_counts().values
plt.pie(sizes, labels=labels, autopct='%1.2f%%', startangle=90, colors=['skyblue', 'lightcoral'])
plt.axis('equal')
plt.title('Pie Chart for Label Distribution')
plt.show()

"""**Data is imbalanced**"""

#checking for the most frequent repeated values
most_frequent_values = df.apply(lambda x: x.value_counts().idxmax())
print("Most frequent values in each column:")
print(most_frequent_values)

"""from the above observation we can see that the value -999 repeated in many column ; most of the time, it can be an error or missing value."""



#checking exacly how many times -999 has repeated
for column in df.columns:
    print(f"Column: {column}")
    print(df[column].value_counts())
    print()

"""**from the above observation we can confirm that -999 is an error or missing data**"""

#replacing -999 with na values
df.replace(-999, np.nan, inplace=True)

df.isnull().sum()/(len(df))*100

null_percentage = df.isnull().mean() * 100

# Ploting a bar graph
plt.figure(figsize=(10, 6))
sns.barplot(x=null_percentage.index, y=null_percentage.values, palette="viridis")
plt.title("Percentage of Null Values in Each Column")
plt.xlabel("Columns")
plt.ylabel("Percentage of Null Values")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()

"""**Since the columns are having more than 30% of missing values we can drop them**"""

#Along with na value columns we will also drop the eventid column since those columns are not going to be useful in this prediction
df.drop(['DER_deltaeta_jet_jet',
           'DER_mass_jet_jet',
           'DER_prodeta_jet_jet',
           'DER_lep_eta_centrality',
           'PRI_jet_leading_pt',
           'PRI_jet_leading_eta',
           'PRI_jet_leading_phi',
           'PRI_jet_subleading_pt',
           'PRI_jet_subleading_eta',
           'PRI_jet_subleading_phi',

           'EventId'], axis=1, inplace=True)

#for column DER_mass_MMC we can fill na values with the median
df['DER_mass_MMC'].fillna(df['DER_mass_MMC'].median(), inplace=True)

df.head()

df.isna().sum()

#Checking for outliers
def bxplot(col):
  sns.boxplot(x=df[col])
  plt.show()

for i in list(df.columns):
  print(bxplot(i))

#Treating the outliers using z-score
z_scores = np.abs((df - df.mean()) / df.std())
threshold = 20
outliers = (z_scores > threshold).any(axis=1)


print("Rows with outliers:")
print(df[outliers])
z_scores

df1 = df[~outliers]

"""**now we apply the label encoding to the Label column**"""

df1['Label'] = df1['Label'].map({'b': 0, 's' : 1})

df1['Label'].value_counts()

#feature target split
x, y = df1.drop('Label', axis=1), df1['Label']

#train test split
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test= train_test_split(x, y, test_size=0.2, random_state=42)
x_train.head()

#balancing the target column by under sampling
from imblearn.under_sampling import RandomUnderSampler
undersampler = RandomUnderSampler(random_state=42)
x_resampled, y_resampled = undersampler.fit_resample(x_train, y_train)

"""# Standardization of the data"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_resampled)
x_test_scaled = scaler.transform(x_test)
y_train=y_resampled

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression
C = 0.0001  # Regularization strength (inverse of alpha, adjust as needed)
lr = LogisticRegression(penalty='l2', C=C, random_state=42)
lr.fit(x_train_scaled, y_train)
y_pred_train = lr.predict(x_train_scaled)
y_pred_test = lr.predict(x_test_scaled)

#checking the accuracy score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
accuracy = accuracy_score(y_test, y_pred_test )
print("Train Accuracy :", accuracy_score(y_train, y_pred_train)*100,'%')
print('Test Accuracy :',accuracy*100,'%')
print()
#classification report
print('Train classification report :')
print( classification_report(y_train, y_pred_train))
print()
print('Test classification report :')
print(classification_report(y_test, y_pred_test))
#confusion matrix
print('Train confusion matrix :')
print(confusion_matrix(y_train, y_pred_train))
print()
print('Test confusion matrix :')
print(confusion_matrix(y_test, y_pred_test))

"""# Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
rforest = RandomForestClassifier(n_estimators=100,
                                 max_depth=1,
                                 min_samples_split=5,
                                 min_samples_leaf=5,
                                 random_state=42)

rforest.fit(x_train_scaled, y_train)
y_pred_train_rf = rforest.predict(x_train_scaled)
y_pred_test_rf = rforest.predict(x_test_scaled)

print("Train Accuracy :", accuracy_score(y_train, y_pred_train_rf)*100,'%')
print("Test Accuracy :", accuracy_score(y_test, y_pred_test_rf)*100,'%')
print()
#classification report
print('Train classification report \n:', classification_report(y_train, y_pred_train_rf))
print('Test classification report :\n', classification_report(y_test, y_pred_test_rf))
#confusion matrix
print('Train confusion matrix :\n', confusion_matrix(y_train, y_pred_train_rf))
print('Test confusion matrix :\n', confusion_matrix(y_test, y_pred_test_rf))

"""# XGBOOST Classifier"""

from xgboost import XGBClassifier
xgboost = XGBClassifier(max_depth=3,
                        min_child_weight=1,
                        subsample=0.1,
                        colsample_bytree=0.1,
                        colsample_bylevel=0.1,
                        n_estimators=10,
                        random_state=42)
xgboost.fit(x_train_scaled, y_train)
y_pred_train_xgb = xgboost.predict(x_train_scaled)
y_pred_test_xgb = xgboost.predict(x_test_scaled)

print("Train Accuracy :", accuracy_score(y_train, y_pred_train_xgb)*100,'%')
print("Test Accuracy :", accuracy_score(y_test, y_pred_test_xgb)*100,'%')
print()
#classification report
print('Train classification report :\n', classification_report(y_train, y_pred_train_xgb))
print('Test classification report :\n', classification_report(y_test, y_pred_test_xgb))
#confusion matrix
print('Train confusion matrix :\n', confusion_matrix(y_train, y_pred_train_xgb))
print('Test confusion matrix :\n', confusion_matrix(y_test, y_pred_test_xgb))

"""#K Nearest Neighbors"""

from sklearn.neighbors import KNeighborsClassifier
knn= KNeighborsClassifier()
knn.fit(x_train_scaled, y_train)
y_pred_train_knn = knn.predict(x_train_scaled)
y_pred_test_knn = knn.predict(x_test_scaled)

#accuracy score
print("Train Accuracy :", accuracy_score(y_train, y_pred_train_knn)*100,'%')
print("Test Accuracy :", accuracy_score(y_test, y_pred_test_knn)*100,'%')
print()
#classification report
print('Train classification report :\n', classification_report(y_train, y_pred_train_knn))
print('Test classification report :\n', classification_report(y_test, y_pred_test_knn))
#confusion matrix
print('Train confusion matrix :\n', confusion_matrix(y_train, y_pred_train_knn))
print('Test confusion matrix :\n', confusion_matrix(y_test, y_pred_test_knn))

"""# Naive Bayes Theorem"""

from sklearn.naive_bayes import BernoulliNB
bnb = BernoulliNB()
bnb.fit(x_train_scaled, y_train)
y_pred_train_bnb = bnb.predict(x_train_scaled)
y_pred_test_bnb = bnb.predict(x_test_scaled)

#accuracy score
print("Train Accuracy :", accuracy_score(y_train, y_pred_train_bnb)*100,'%')
print("Test Accuracy :", accuracy_score(y_test, y_pred_test_bnb)*100,'%')
print()
#classification report
print('Train classification report :\n', classification_report(y_train, y_pred_train_bnb))
print('Test classification report :\n', classification_report(y_test, y_pred_test_bnb))
#confusion matrix
print('Train confusion matrix :\n', confusion_matrix(y_train, y_pred_train_bnb))
print('Test confusion matrix :\n', confusion_matrix(y_test, y_pred_test_bnb))

"""# Stacking method"""

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import BernoulliNB
from xgboost import XGBClassifier

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Base models
base_models = [
    ('lr', LogisticRegression(penalty='l2', C=C, random_state=42)),

    ('knn', KNeighborsClassifier()),
    ('xgb', XGBClassifier(max_depth=3,
                        min_child_weight=1,
                        subsample=0.1,
                        colsample_bytree=0.1,
                        colsample_bylevel=0.1,
                        n_estimators=10,
                        random_state=42)),
    ('rforest', RandomForestClassifier(n_estimators=100,
                                 max_depth=1,
                                 min_samples_split=5,
                                 min_samples_leaf=5,
                                 random_state=42)),
    ('bnb', BernoulliNB())]

# Meta-model (Logistic Regression is used)
meta_model = LogisticRegression()

# Creating the stacking model
stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)


stacking_model.fit(x_train_scaled, y_train)
y_pred_train_stacking = stacking_model.predict(x_train_scaled)
y_pred_test_stacking = stacking_model.predict(x_test_scaled)


# Evaluating the stacking model
accuracy_stacking_train = accuracy_score(y_train, y_pred_train_stacking)
print("Stacking Model Train Accuracy:", accuracy_stacking_train * 100, '%')
accuracy_stacking = accuracy_score(y_test, y_pred_test_stacking)
print("Stacking Model Test Accuracy:", accuracy_stacking * 100, '%')

#classification report
print('Train classification report :\n', classification_report(y_train, y_pred_train_stacking))
print('Test classification report :\n', classification_report(y_test, y_pred_test_stacking))


#confusion matrix
print('Train confusion matrix :\n', confusion_matrix(y_train, y_pred_train_stacking))
print('Test confusion matrix :\n', confusion_matrix(y_test, y_pred_test_stacking))

"""# Cross Validation"""

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import make_scorer

num_folds = 5

stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)


cv_strategy = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

scorer = make_scorer(accuracy_score)

# Perform cross-validation
cv_scores = cross_val_score(stacking_model, x_train_scaled, y_train, cv=cv_strategy, scoring=scorer)


print(f"Cross-Validation Scores (Accuracy): {cv_scores}")
print(f"Mean Cross-Validation Accuracy: {cv_scores.mean() * 100:.2f}%")
print(f"Standard Deviation of Cross-Validation Accuracy: {cv_scores.std() * 100:.2f}%")